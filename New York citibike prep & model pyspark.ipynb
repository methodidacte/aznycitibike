{"cells":[{"cell_type":"markdown","source":["<p>\n  <a href=\"$./Clean Data Citibike CSV. (1)\">Clener code at this location</a>\n</p>\n<p>\n  <a href=\"$./Random Forests\">Random Forest regression</a>\n</p>\n<p>\n  <a href=\"$./Decision tree regression\">Decision tree regression</a>\n</p>\n<p>\n  <a href=\"$./Linear Regression\">Linear Regression</a>\n</p>\n<p>\n  <a href=\"$./Gradient-boosted tree regression\">Gradient-boosted tree regression</a>\n</p>"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import udf\nfrom pyspark.sql import functions as F\nfrom math import acos, atan, cos, pi, radians, sin, sqrt\nfrom pyspark.sql.types import StructType, StructField, IntegerType, DoubleType, StringType, TimestampType"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["@udf(\"float\")\ndef Distance(lat1, long1, lat2, long2, unite):\n    \"\"\"\n    Calculates the distance between two points with latitude and longitude as input parameters.\n    'lat1, long1, lat2, long2,' int in str or float type.\n    'unite' str type.\n    \"\"\"\n    # Convert in miles or in km by default.\n    if unite == 'miles':\n        convert = 1.60934\n    else:\n        convert = 1\n    \n    # Set latitude and longitude in float().\n    lat1 = float(lat1)\n    long1 = float(long1)\n    lat2 = float(lat2)\n    long2 = float(long2)\n    \n    # Set latitude and longitude in radian.\n    lat1rad = lat1 * pi / 180\n    long1rad = long1 * pi / 180\n    lat2rad = lat2 * pi / 180\n    long2rad = long2 * pi / 180\n    \n    #Radius of the earth\n    r = 6372.797\n    \n    # Radian latitude2 minus latitude\n    dlat = lat2rad - lat1rad\n    \n    # Radian longitude2 minus longitude\n    dlng = long2rad - long1rad\n    \n    a = sin(dlat / 2) * sin(dlat / 2) + cos(lat1) * cos(lat2) * sin(dlng / 2) * sin(dlng / 2)\n    try:\n        c = 2 * atan(sqrt(a) / sqrt(1 - a))\n    except:\n        # if a < 0 c funtion break, return 0 and drop line if distance == 0.\n        # a < 0 when bike is in repaire center, latitude and longitude is set at 0.\n        return 0\n        \n    km = r * c\n    \n    return km/convert"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["def rename_columns(df, columns):\n  \"\"\"\n  Rename all columns present in variable 'columns' in the dataset 'df.\n  \"\"\"\n  if isinstance(columns, dict):\n      for old_name, new_name in columns.items():\n          df = df.withColumnRenamed(old_name, new_name)\n      return df\n  else:\n      raise ValueError(\"'columns' should be a dict, like {'old_name_1':'new_name_1', 'old_name_2':'new_name_2'}\")"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["def ColToDummies(df, columns):\n  \"\"\"\n  Take dataset in parameter 'df' and column 'columns'.\n  For this column look all distinct values.\n  Create columns with all distinct values and for each row if 'column' values is equal to the new column name take as values 1 else 0.\n  \"\"\"\n  categories = df.select(columns).distinct().rdd.flatMap(lambda x: x).collect()\n\n  exprs = [F.when(F.col(columns) == category, 1).otherwise(0).alias(category)\n           for category in categories]\n  return exprs"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["from pyspark.ml.feature import OneHotEncoder, StringIndexer\n\ndef ColToDummiesOHE(df, column):\n\n  stringIndexer = StringIndexer(inputCol=column, outputCol=column+\"Index\")\n  model = stringIndexer.fit(df)\n  indexed = model.transform(df)\n\n  encoder = OneHotEncoder(inputCol=column+\"Index\", outputCol=column+\"Vec\")\n  encoded = encoder.transform(indexed)\n\n  return encoded"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["# Set all variables\nlocation_field = \"/mnt/nycitibike/CSV/*/*\"\nlocation_field_date = \"/mnt/nycitibike/CSV/\"\nformat_field = \"csv\"\n\n\n# Set schema for csv to dataset.\nschema = StructType([\n    StructField(\"trip_duration\", IntegerType(), True),\n    StructField(\"start_time\", TimestampType(), True),\n    StructField(\"stop_time\", TimestampType(), True),\n    StructField(\"start_station_id\", IntegerType(), True),\n    StructField(\"start_station_name\", StringType(), True),\n    StructField(\"start_station_latitude\", DoubleType(), True),\n    StructField(\"start_station_longitude\", DoubleType(), True),\n    StructField(\"end_station_id\", IntegerType(), True),\n    StructField(\"end_station_name\", StringType(), True),\n    StructField(\"end_station_latitude\", DoubleType(), True),\n    StructField(\"end_station_longitude\", DoubleType(), True),\n    StructField(\"bike_id\", IntegerType(), True),\n    StructField(\"usertype\", StringType(), True),\n    StructField(\"birth_year\", IntegerType(), True),\n    StructField(\"gender\", StringType(), True),    \n    ])"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["# Show all files in path \ndisplay(dbutils.fs.ls(location_field_date))"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["# Take all file in path 'location_field' with extention 'format_field'.load(location_field).\n# .option(\"header\",\"true\") each files have header, pass this line.\n# .option(\"mode\", \"DROPMALFORMED\") Drop each lines dosen't have  the current schema.\n# .schema(schema) override in parameter columns type and column name.\n\ndf = spark.read.format(format_field).option(\"header\",\"true\").option(\"mode\", \"DROPMALFORMED\").schema(schema).load(location_field)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["# Take the first line and print it.\ndf.head()"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["# Use display() on Databricks\ndisplay(df)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["df_raw = df"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["df_raw.count()"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["#https://stackoverflow.com/questions/44627386/how-to-find-count-of-null-and-nan-values-for-each-column-in-a-pyspark-dataframe\n\nfrom pyspark.sql.functions import isnan, when, count, col\n\n#df_raw.select([count(when(isnan(c), c)).alias(c) for c in df_raw.columns]).show()\n#df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df_raw.columns]).show()\ndf_raw.select('usertype').withColumn('isNull_c',F.col('usertype').isNull()).where('isNull_c = True').count() #51780"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["# Create dataframe without empty values\n# Dropna with parameter how set to any drop all row with empty value.\ndf = df.dropna(how=\"any\")"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["# New dataFrame with only 'trip_duration' values lower than 50 minutes. \ndf = df.where((df.trip_duration <= 3000 ))"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["%time\ndf1 = df.select(\"trip_duration\",\"start_time\",\"stop_time\",\"start_station_id\",\"start_station_name\",\"start_station_latitude\",\"start_station_longitude\",\"end_station_id\",\"end_station_name\",\"end_station_latitude\",\"end_station_longitude\",\"bike_id\",\"usertype\",\"birth_year\", *ColToDummies(df,\"gender\"))"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["%time\ndf2 = ColToDummiesOHE(df,\"gender\")"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["df = df1\ndf = rename_columns(df, {\"0\":\"unknown_gender\",\"1\":\"male_gender\",\"2\":\"female_gender\"})"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["df = df.select(\"trip_duration\",\"start_time\",\"stop_time\",\"start_station_id\",\"start_station_name\",\"start_station_latitude\",\"start_station_longitude\",\"end_station_id\",\"end_station_name\",\"end_station_latitude\",\"end_station_longitude\",\"bike_id\",\"birth_year\",\"unknown_gender\",\"male_gender\",\"female_gender\", *ColToDummies(df,\"usertype\"))"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["df = df.withColumn('distance_bwn_stations', Distance(\"start_station_latitude\",\"start_station_longitude\",\"end_station_latitude\",\"end_station_longitude\",F.lit(\"km\")))"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["# New dataFrame with only 'distance_bwn_stations' values greater than 0.0 km. \ndf = df.where((df.distance_bwn_stations > 0))"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["df.write.format(\"parquet\").saveAsTable(\"CitibikeNY\")"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["%sql\n\nSELECT COUNT(1), MIN(start_time), MAX(stop_time) FROM CitibikeNY"],"metadata":{},"outputs":[],"execution_count":24}],"metadata":{"name":"Clean Data Citibike CSV. (1)","notebookId":935421876010899},"nbformat":4,"nbformat_minor":0}
